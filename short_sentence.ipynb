{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46588, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from src.article_predict import split_sentence\n",
    "\n",
    "origin_df = pd.read_excel('data/article/train_augmented_1113_SSL.xlsx').fillna(\"\")\n",
    "df = pd.read_excel(\"data/raw/new_clean_data_all.xlsx\")\n",
    "print(df.shape)\n",
    "test_article = pd.read_excel(\"data/article/test_article.xlsx\").fillna(\"\")['TextID']\n",
    "df = df[~df['TextID'].isin(test_article)]\n",
    "short_sentences_1 = df[df['自殺行為'] == 1]['Sentence']\n",
    "short_sentences_2 = df[df['自殺與憂鬱'] == 1]['Sentence']\n",
    "# print(short_sentences[:5].values)\n",
    "# short_sentences = df[df['自殺行為'] == 1]['Sentence']\n",
    "# print(short_sentences[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random choose 100 sentences from short_sentences\n",
    "import random\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "add_num_1 = 100\n",
    "\n",
    "random_lines = random.sample(sorted(short_sentences_1.values), add_num_1)\n",
    "textID_list = [ ('short_article_' + str(i)) for i in range(add_num_1)]\n",
    "article_dict = {str(key): {'無標註':'', '自殺與憂鬱':'',\n",
    "       '無助或無望':'', '正向文字':'', '其他負向文字':'', '生理反應或醫療狀況':'', '自殺行為':'', '其他類型':'', 'label':[0.0, 0.0, 0.0, 1.0], 'Crisis_Level':3, 'TextID':str(key)} for key in textID_list}\n",
    "for idx, arti in enumerate(article_dict):\n",
    "    article_dict[arti]['自殺行為'] = random_lines[idx]\n",
    "all_article = []\n",
    "for dic in article_dict:\n",
    "    single_article = pd.DataFrame([article_dict[dic]])\n",
    "    single_article['TextID'] = dic\n",
    "    all_article.append(single_article)\n",
    "# all_article_df = pd.concat(all_article,axis=0, ignore_index=True)\n",
    "# print(all_article_df.columns)\n",
    "# print(all_article_df.shape)\n",
    "# all_article_df['TextID'] = all_article_df['TextID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['無標註', '自殺與憂鬱', '無助或無望', '正向文字', '其他負向文字', '生理反應或醫療狀況', '自殺行為', '其他類型',\n",
      "       'label', 'Crisis_Level', 'TextID'],\n",
      "      dtype='object')\n",
      "(100, 11)\n"
     ]
    }
   ],
   "source": [
    "# random choose 100 sentences from short_sentences\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "add_num_2 = 0\n",
    "\n",
    "random_lines = random.sample(sorted(short_sentences_2.values), add_num_2)\n",
    "textID_list = [ ('short_article_' + str(i)) for i in range(add_num_2)]\n",
    "article_dict = {str(key): {'無標註':'', '自殺與憂鬱':'',\n",
    "       '無助或無望':'', '正向文字':'', '其他負向文字':'', '生理反應或醫療狀況':'', '自殺行為':'', '其他類型':'', 'label':[0.0, 0.0, 1.0, 0.0], 'Crisis_Level':2, 'TextID':str(key)} for key in textID_list}\n",
    "for idx, arti in enumerate(article_dict):\n",
    "    article_dict[arti]['自殺與憂鬱'] = random_lines[idx]\n",
    "# all_article = []\n",
    "for dic in article_dict:\n",
    "    single_article = pd.DataFrame([article_dict[dic]])\n",
    "    single_article['TextID'] = dic\n",
    "    all_article.append(single_article)\n",
    "all_article_df = pd.concat(all_article,axis=0, ignore_index=True)\n",
    "print(all_article_df.columns)\n",
    "print(all_article_df.shape)\n",
    "all_article_df['TextID'] = all_article_df['TextID'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', '無標註', '自殺與憂鬱', '自殺行為', '其他類型', 'label', 'Crisis_Level',\n",
      "       'TextID'],\n",
      "      dtype='object') (1820, 8)\n",
      "Index(['無標註', '自殺與憂鬱', '自殺行為', '其他類型', 'label', 'Crisis_Level', 'TextID'], dtype='object') (1820, 7)\n"
     ]
    }
   ],
   "source": [
    "print(origin_df.columns, origin_df.shape)\n",
    "# remove column 'Unnamed: 0' in origin_df\n",
    "origin_df = origin_df.drop(columns=['Unnamed: 0'])\n",
    "print(origin_df.columns, origin_df.shape)\n",
    "augmented_df = pd.concat([origin_df, all_article_df],axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    500\n",
      "1    500\n",
      "3    483\n",
      "2    437\n",
      "Name: Crisis_Level, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(augmented_df['Crisis_Level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df.to_excel('data/article/train_augmented_short_1113_SSL.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
